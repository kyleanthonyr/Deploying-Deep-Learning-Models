{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters\n",
    "Without going into too much theory or testing many different values, here we use standard values for the batch size (which defines the number of training samples to work through before updating the model weights) and number of epochs (full presentations of the data in the training set for learning). There are 10 classes since we're considering the digits 1-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparams\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Images\n",
    "The next step is to load our data set and set constant image sizes for our training process. The images sizes are fixed to (28 x 28), as the network input parameters are always constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Resolution\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Loading the data\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing\n",
    "In this step we need to make sure that the training data is pre-processed and tuned to the same direction; if your inputs are of different sizes, the performance of your network will be inaccurate. \n",
    "\n",
    "We use a simple reshape method on every image and iterate it over the complete data set. Next, we assign the respected label to every image for the training process, in this case, we use the to_categorical method to assign a label to every image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Model \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.legacy.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 2.2831 - accuracy: 0.1324 - val_loss: 2.2567 - val_accuracy: 0.1802\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 2.2393 - accuracy: 0.2162 - val_loss: 2.2024 - val_accuracy: 0.3588\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 2.1831 - accuracy: 0.3039 - val_loss: 2.1296 - val_accuracy: 0.4534\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 2.1053 - accuracy: 0.3836 - val_loss: 2.0303 - val_accuracy: 0.5564\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 2.0024 - accuracy: 0.4584 - val_loss: 1.8969 - val_accuracy: 0.6470\n",
      "Epoch 6/12\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 1.8689 - accuracy: 0.5224 - val_loss: 1.7268 - val_accuracy: 0.7176\n",
      "Epoch 7/12\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 1.7107 - accuracy: 0.5706 - val_loss: 1.5324 - val_accuracy: 0.7610\n",
      "Epoch 8/12\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 1.5436 - accuracy: 0.6065 - val_loss: 1.3354 - val_accuracy: 0.7896\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 1.3849 - accuracy: 0.6374 - val_loss: 1.1564 - val_accuracy: 0.8095\n",
      "Epoch 10/12\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 1.2541 - accuracy: 0.6604 - val_loss: 1.0092 - val_accuracy: 0.8245\n",
      "Epoch 11/12\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 1.1384 - accuracy: 0.6791 - val_loss: 0.8907 - val_accuracy: 0.8350\n",
      "Epoch 12/12\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 1.0526 - accuracy: 0.6974 - val_loss: 0.7997 - val_accuracy: 0.8440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x294017010>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7997066378593445\n",
      "Test accuracy: 0.843999981880188\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the Predictions on the Model\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model for Future Inferences\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"../models/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"../models/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
